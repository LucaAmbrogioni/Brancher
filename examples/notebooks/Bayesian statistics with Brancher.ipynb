{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics with Brancher\n",
    "\n",
    "Computers are glorified calculators. Similarly, probabilistic programming is glorified (usually Bayesian) statistics. Therefore, the simplest use of Brancher is as an agile and intuitive toolbox for performing Bayesian inference in few variables.\n",
    "\n",
    "The starting point of all Bayesian models is a probabilistic generative model:\n",
    "\n",
    " $$ p(x|z) p(z)~, $$\n",
    " \n",
    "where $x$ is an observabla variable and $z$ is a latent variable. \n",
    "In this model, $p(x|z)$ (the likelihood) is basically a recipe for generating data $x$ as a stochastic function of an unobservable variable $z$. On the other hand, $p(z)$ (the prior) represents your knowledge of $z$ before observing $x$.  \n",
    "\n",
    "The goal of Baysian inference is to obtain the posterior distribution of $z$ given the model and the observed value of $x$. This posterior is given by the Bayes theorem:\n",
    "\n",
    "$$ p(z|x) = \\frac{p(x| z) p(z)}{p(x)}~. $$\n",
    "\n",
    "It is not trivial to directly use this formula in most real-wold problems (the tricky bit is the innocuous looking term $p(x)$). Brancher deals with this difficulty using an approximate technique called \\amph{stochastic variational inference}. We we deal with the inference later on. For now, let's see how to write generative models in Brancher:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulding the Beta Binomial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On of the simplest example of generative model is the Beta-Binomial model. Imagine that you are tring to determine is a coin is fair by tossing it $n = 10$ times. Assume that $\\rho$ is the (unobservable) true probability of the coin landing on head. The probability of observing head $k$ times is given by a Binomial distribution:\n",
    "\n",
    "$$\n",
    "p(k|\\rho) = \\text{Binomial}\\left(n, \\rho \\right)~.\n",
    "$$\n",
    "\n",
    "Now that the likelihood has been specified, we can complete the generative model by specifying a prior. The only strict requirement here is that the probability (density) of $\\rho$ should be cointained in the interval $[0,1]$ since $\\rho$ is a probability. A mathematically convevient choice is the Beta distribution:\n",
    "\n",
    "$$\n",
    "\\rho \\sim \\text{Beta}\\left(a, b \\right)~.\n",
    "$$\n",
    "\n",
    "Let's now write this model in Brancher. The first step is to import the relevant classes of random variables from the standard_variables submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.standard_variables import BinomialVariable as Binomial\n",
    "from brancher.standard_variables import BetaVariable as Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brancher, Beta and Binomial are classes that can be used for initializing the random variables that will form the building blocks of our generative model. For example, let's initialize the $\\rho$ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = Beta(alpha=1., beta=1., name=\"rho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ and $\\beta$ are parameters of the Beta distribution. Now that we have $\\rho$, we can initialize $k$ as a binomial variable that depends on $\\rho$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Binomial(n=10, p=rho, name=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes our simple model! We just need to insert the variables we constreucted in a ProbabilisticModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.variables import ProbabilisticModel \n",
    "\n",
    "model = ProbabilisticModel([rho, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done, the generative model is now fully encoded as a brancher object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we did not specify that the number of heads $k$ has been oberved. Let's say that we got $8$ heads, we can tell this to brancher by calling the \"observe\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.observe(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting ready for the inference: the variational model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to determine the probability of $\\rho$ (yes it is the probability of a probability, a bit confusing...) given the fact that we observed 8 heads. \n",
    "\n",
    "Brancher can solve this problem for you by fitting the parameter of an approximate posterior model. We will refer to this model as the \"variational model\". We need to create a variational model that contains all the non-observed variables in your generative model. Variational models are just probabilistic models and can be created in the very same way. The simplest way to make an appropriate model is to \"copy the prior\". More formally, we create an approximate posterior that follows the same distribution of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qrho = Beta(alpha=1., beta=1., name=\"rho\", learnable=True)\n",
    "variational_model = ProbabilisticModel([Qrho])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference here is that we set the learnable flag as True. This is important since we need to fit the parameters alpha and beta of Qrho in order to approximate the real posterior. We can now set our variational model as approximate posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_posterior_model(variational_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the work is done! Now we can just ask brancher to fit the parameters of the posterior model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from brancher import inference \n",
    "\n",
    "inference.perform_inference(model,\n",
    "                            number_iterations=3000,\n",
    "                            number_samples=10,\n",
    "                            lr=0.1,\n",
    "                            optimizer='Adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wo get the loss and plot it to see if the posterior parameters converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_list = model.diagnostics[\"loss curve\"]\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.visualizations import plot_posterior\n",
    "\n",
    "plot_posterior(model, variables=[\"rho\"]) #TODO: to be fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:UserBrancher]",
   "language": "python",
   "name": "conda-env-UserBrancher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
